{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fdc355-4efb-417d-8f65-7d0e5ba6fb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, TrainingArguments, Trainer\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Load datasets\n",
    "df_train = pd.read_csv('df_train4.csv')\n",
    "df_valid = pd.read_csv('df_valid4.csv')\n",
    "df_test = pd.read_csv('df_test4.csv')\n",
    "\n",
    "# Combine train and validation datasets\n",
    "df_combined = pd.concat([df_train, df_valid], ignore_index=True)\n",
    "\n",
    "# Ensure labels are integers\n",
    "df_combined['label'] = df_combined['label'].astype(int)\n",
    "df_test['label'] = df_test['label'].astype(int)\n",
    "\n",
    "# Define metrics function\n",
    "def compute_metrics(p):\n",
    "    preds = p.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(p.label_ids, preds, average='weighted')\n",
    "    accuracy = accuracy_score(p.label_ids, preds)\n",
    "    return {'accuracy': accuracy, 'f1': f1, 'precision': precision, 'recall': recall}\n",
    "\n",
    "# Model names\n",
    "model_names = [\n",
    "    \"w11wo/javanese-bert-small-imdb-classifier\",\n",
    "    \"w11wo/javanese-gpt2-small-imdb-classifier\",\n",
    "    \"w11wo/javanese-distilbert-small-imdb-classifier\"\n",
    "]\n",
    "\n",
    "# Fixed hyperparameters\n",
    "learning_rate = 5e-5\n",
    "batch_size = 16\n",
    "num_epochs = 3\n",
    "\n",
    "k_folds = 5\n",
    "skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Store cross-validation results for each model\n",
    "cross_val_results = []\n",
    "\n",
    "# Perform cross-validation for each model\n",
    "for model_name in model_names:\n",
    "    print(f\"\\nCross-validating model: {model_name}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    fold_metrics = []\n",
    "    \n",
    "    # Prepare datasets for each fold\n",
    "    for fold, (train_index, val_index) in enumerate(skf.split(df_combined['sentence'], df_combined['label'])):\n",
    "        print(f\"Fold {fold+1}\")\n",
    "        \n",
    "        train_data = df_combined.iloc[train_index]\n",
    "        val_data = df_combined.iloc[val_index]\n",
    "        \n",
    "        # Convert to Hugging Face datasets\n",
    "        train_dataset = Dataset.from_pandas(train_data)\n",
    "        val_dataset = Dataset.from_pandas(val_data)\n",
    "        \n",
    "        # Preprocess data\n",
    "        def preprocess_function(examples):\n",
    "            return tokenizer(examples['sentence'], truncation=True, padding='max_length', max_length=128)\n",
    "        \n",
    "        tokenized_train = train_dataset.map(preprocess_function, batched=True)\n",
    "        tokenized_val = val_dataset.map(preprocess_function, batched=True)\n",
    "        \n",
    "        data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "        \n",
    "        # Load model\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=4, ignore_mismatched_sizes=True)\n",
    "        \n",
    "        # Define training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f'./results/{model_name}/fold_{fold}',\n",
    "            learning_rate=learning_rate,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            num_train_epochs=num_epochs,\n",
    "            weight_decay=0.01,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"no\",\n",
    "            load_best_model_at_end=False,\n",
    "            push_to_hub=False\n",
    "        )\n",
    "        \n",
    "        # Initialize Trainer\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_train,\n",
    "            eval_dataset=tokenized_val,\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=compute_metrics\n",
    "        )\n",
    "        \n",
    "        # Train the model\n",
    "        trainer.train()\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        val_results = trainer.predict(tokenized_val)\n",
    "        fold_metrics.append(compute_metrics(val_results))\n",
    "    \n",
    "    # Average metrics for the current model\n",
    "    avg_metrics = {\n",
    "        'Model': model_name,\n",
    "        'Accuracy': np.mean([m['accuracy'] for m in fold_metrics]),\n",
    "        'F1 Score': np.mean([m['f1'] for m in fold_metrics]),\n",
    "        'Precision': np.mean([m['precision'] for m in fold_metrics]),\n",
    "        'Recall': np.mean([m['recall'] for m in fold_metrics])\n",
    "    }\n",
    "    cross_val_results.append(avg_metrics)\n",
    "\n",
    "# Create a DataFrame for the cross-validation results\n",
    "cross_val_results_df = pd.DataFrame(cross_val_results)\n",
    "\n",
    "# Display the cross-validation results table\n",
    "print(\"\\nCross-Validation Results:\")\n",
    "print(cross_val_results_df)\n",
    "\n",
    "# Train on combined data and evaluate on the test set\n",
    "test_results = []\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f\"\\nTraining and evaluating {model_name} on the test set.\")\n",
    "    \n",
    "    # Combine the train and validation data for the model\n",
    "    train_dataset = Dataset.from_pandas(df_combined)\n",
    "    \n",
    "    # Preprocess the combined dataset\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    def preprocess_function(examples):\n",
    "        return tokenizer(examples['sentence'], truncation=True, padding='max_length', max_length=128)\n",
    "    \n",
    "    tokenized_train = train_dataset.map(preprocess_function, batched=True)\n",
    "    \n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    \n",
    "    # Load the model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=4, ignore_mismatched_sizes=True)\n",
    "    \n",
    "    # Define training arguments for final training\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f'./results/{model_name}/final',\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=num_epochs,\n",
    "        weight_decay=0.01,\n",
    "        push_to_hub=False\n",
    "    )\n",
    "    \n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    \n",
    "    # Train on the full combined dataset\n",
    "    trainer.train()\n",
    "    \n",
    "    # Preprocess test data\n",
    "    test_dataset = Dataset.from_pandas(df_test)\n",
    "    tokenized_test = test_dataset.map(lambda x: tokenizer(x['sentence'], truncation=True, padding='max_length', max_length=128), batched=True)\n",
    "    \n",
    "    # Evaluate on the test set\n",
    "    test_results_output = trainer.predict(tokenized_test)\n",
    "    test_metrics = compute_metrics(test_results_output)\n",
    "    \n",
    "    # Store the test results\n",
    "    test_results.append({\n",
    "        'Model': model_name,\n",
    "        'Test Accuracy': test_metrics['accuracy'],\n",
    "        'Test F1 Score': test_metrics['f1'],\n",
    "        'Test Precision': test_metrics['precision'],\n",
    "        'Test Recall': test_metrics['recall']\n",
    "    })\n",
    "\n",
    "# Create a DataFrame for the final results\n",
    "test_results_df = pd.DataFrame(test_results)\n",
    "\n",
    "# Display the final test results table\n",
    "print(\"\\nTest Set Evaluation Results:\")\n",
    "print(test_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc839b9d-8c4d-4faa-9c37-83ff68964624",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb8893c-92ce-4c3b-ac69-60e0462400a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f268b8c7-dbd4-4065-a0b0-e280268b8a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, TrainingArguments, Trainer\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def load_data():\n",
    "    df_train = pd.read_csv('df_train4.csv')\n",
    "    df_valid = pd.read_csv('df_valid4.csv')\n",
    "    df_test = pd.read_csv('df_test4.csv')\n",
    "    df_combined = pd.concat([df_train, df_valid], ignore_index=True)\n",
    "    df_combined['label'] = df_combined['label'].astype(int)\n",
    "    df_test['label'] = df_test['label'].astype(int)\n",
    "    return df_combined, df_test\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = p.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(p.label_ids, preds, average='weighted')\n",
    "    accuracy = accuracy_score(p.label_ids, preds)\n",
    "    return {'accuracy': accuracy, 'f1': f1, 'precision': precision, 'recall': recall}\n",
    "\n",
    "def preprocess_function(examples, tokenizer):\n",
    "    return tokenizer(examples['sentence'], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "def train_and_evaluate_model(model_name, df_combined, skf):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    fold_metrics = []\n",
    "    for fold, (train_index, val_index) in enumerate(skf.split(df_combined['sentence'], df_combined['label'])):\n",
    "        train_data = df_combined.iloc[train_index]\n",
    "        val_data = df_combined.iloc[val_index]\n",
    "        train_dataset = Dataset.from_pandas(train_data)\n",
    "        val_dataset = Dataset.from_pandas(val_data)\n",
    "        tokenized_train = train_dataset.map(lambda x: preprocess_function(x, tokenizer), batched=True)\n",
    "        tokenized_val = val_dataset.map(lambda x: preprocess_function(x, tokenizer), batched=True)\n",
    "        data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=4, ignore_mismatched_sizes=True)\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f'/scratch/lf93/iw/cv_results/{model_name}/fold_{fold}',\n",
    "            learning_rate=5e-5,\n",
    "            per_device_train_batch_size=16,\n",
    "            per_device_eval_batch_size=16,\n",
    "            num_train_epochs=3,\n",
    "            weight_decay=0.01,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"no\",\n",
    "            load_best_model_at_end=False,\n",
    "            push_to_hub=False\n",
    "        )\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_train,\n",
    "            eval_dataset=tokenized_val,\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=compute_metrics\n",
    "        )\n",
    "        trainer.train()\n",
    "        val_results = trainer.predict(tokenized_val)\n",
    "        fold_metrics.append(compute_metrics(val_results))\n",
    "    avg_metrics = {\n",
    "        'Model': model_name,\n",
    "        'Accuracy': np.mean([m['accuracy'] for m in fold_metrics]),\n",
    "        'F1 Score': np.mean([m['f1'] for m in fold_metrics]),\n",
    "        'Precision': np.mean([m['precision'] for m in fold_metrics]),\n",
    "        'Recall': np.mean([m['recall'] for m in fold_metrics])\n",
    "    }\n",
    "    return avg_metrics\n",
    "\n",
    "def cross_validate_models(model_names, df_combined, k_folds):\n",
    "    skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "    cross_val_results = []\n",
    "    for model_name in model_names:\n",
    "        print(f\"\\nCross-validating model: {model_name}\")\n",
    "        avg_metrics = train_and_evaluate_model(model_name, df_combined, skf)\n",
    "        cross_val_results.append(avg_metrics)\n",
    "    return pd.DataFrame(cross_val_results)\n",
    "\n",
    "def train_final_models(model_names, df_combined, df_test):\n",
    "    test_results = []\n",
    "    for model_name in model_names:\n",
    "        print(f\"\\nTraining and evaluating {model_name} on the test set.\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        train_dataset = Dataset.from_pandas(df_combined)\n",
    "        tokenized_train = train_dataset.map(lambda x: preprocess_function(x, tokenizer), batched=True)\n",
    "        data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=4, ignore_mismatched_sizes=True)\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f'/scratch/lf93/iw/cv_results/{model_name}/final',\n",
    "            learning_rate=5e-5,\n",
    "            per_device_train_batch_size=16,\n",
    "            per_device_eval_batch_size=16,\n",
    "            num_train_epochs=3,\n",
    "            weight_decay=0.01,\n",
    "            push_to_hub=False\n",
    "        )\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_train,\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=compute_metrics\n",
    "        )\n",
    "        trainer.train()\n",
    "        test_dataset = Dataset.from_pandas(df_test)\n",
    "        tokenized_test = test_dataset.map(lambda x: preprocess_function(x, tokenizer), batched=True)\n",
    "        test_results_output = trainer.predict(tokenized_test)\n",
    "        test_metrics = compute_metrics(test_results_output)\n",
    "        test_results.append({\n",
    "            'Model': model_name,\n",
    "            'Test Accuracy': test_metrics['accuracy'],\n",
    "            'Test F1 Score': test_metrics['f1'],\n",
    "            'Test Precision': test_metrics['precision'],\n",
    "            'Test Recall': test_metrics['recall']\n",
    "        })\n",
    "    return pd.DataFrame(test_results)\n",
    "\n",
    "# Main execution\n",
    "df_combined, df_test = load_data()\n",
    "\n",
    "model_names = [\n",
    "    \"w11wo/javanese-bert-small-imdb-classifier\",\n",
    "    \"w11wo/javanese-gpt2-small-imdb-classifier\",\n",
    "    \"w11wo/javanese-distilbert-small-imdb-classifier\"\n",
    "]\n",
    "\n",
    "cross_val_results_df = cross_validate_models(model_names, df_combined, k_folds=5)\n",
    "print(\"\\nCross-Validation Results:\")\n",
    "print(cross_val_results_df)\n",
    "\n",
    "test_results_df = train_final_models(model_names, df_combined, df_test)\n",
    "print(\"\\nTest Set Evaluation Results:\")\n",
    "print(test_results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8da3d3-228c-46fa-bcd6-5af491ae5b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5c4d1a-7b6b-4ba4-98cb-ae42550dab57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "\n",
    "def load_data():\n",
    "    df_train = pd.read_csv('df_train4.csv')\n",
    "    df_valid = pd.read_csv('df_valid4.csv')\n",
    "    df_test = pd.read_csv('df_test4.csv')\n",
    "    df_combined = pd.concat([df_train, df_valid], ignore_index=True)\n",
    "    df_combined['label'] = df_combined['label'].astype(int)\n",
    "    df_test['label'] = df_test['label'].astype(int)\n",
    "    return df_combined, df_test\n",
    "\n",
    "def compute_metrics(p):\n",
    "    y_true = p.label_ids\n",
    "    y_pred = p.predictions.argmax(-1)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'confusion_matrix': conf_matrix\n",
    "    }\n",
    "\n",
    "def preprocess_function(examples, tokenizer):\n",
    "    return tokenizer(examples['sentence'], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "def train_and_evaluate_model(model_name, df_combined, skf):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    fold_metrics = []\n",
    "    for fold, (train_index, val_index) in enumerate(skf.split(df_combined['sentence'], df_combined['label'])):\n",
    "        train_data = df_combined.iloc[train_index]\n",
    "        val_data = df_combined.iloc[val_index]\n",
    "        train_dataset = Dataset.from_pandas(train_data)\n",
    "        val_dataset = Dataset.from_pandas(val_data)\n",
    "        tokenized_train = train_dataset.map(lambda x: preprocess_function(x, tokenizer), batched=True)\n",
    "        tokenized_val = val_dataset.map(lambda x: preprocess_function(x, tokenizer), batched=True)\n",
    "        data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=4, ignore_mismatched_sizes=True)\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f'/scratch/lf93/iw/cv_results/{model_name}/fold_{fold}',\n",
    "            learning_rate=5e-5,\n",
    "            per_device_train_batch_size=16,\n",
    "            per_device_eval_batch_size=16,\n",
    "            num_train_epochs=3,\n",
    "            weight_decay=0.01,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"no\",\n",
    "            load_best_model_at_end=False,\n",
    "            push_to_hub=False\n",
    "        )\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_train,\n",
    "            eval_dataset=tokenized_val,\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=compute_metrics\n",
    "        )\n",
    "        trainer.train()\n",
    "        val_results = trainer.predict(tokenized_val)\n",
    "        fold_metrics.append(compute_metrics(val_results))\n",
    "    avg_metrics = {\n",
    "        'Model': model_name,\n",
    "        'Accuracy': np.mean([m['accuracy'] for m in fold_metrics]),\n",
    "        'F1 Score': np.mean([m['f1'] for m in fold_metrics]),\n",
    "        'Precision': np.mean([m['precision'] for m in fold_metrics]),\n",
    "        'Recall': np.mean([m['recall'] for m in fold_metrics]),\n",
    "        'Confusion Matrices': [m['confusion_matrix'] for m in fold_metrics]\n",
    "    }\n",
    "    return avg_metrics\n",
    "\n",
    "def cross_validate_models(model_names, df_combined, k_folds):\n",
    "    skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "    cross_val_results = []\n",
    "    for model_name in model_names:\n",
    "        print(f\"\\nCross-validating model: {model_name}\")\n",
    "        avg_metrics = train_and_evaluate_model(model_name, df_combined, skf)\n",
    "        cross_val_results.append(avg_metrics)\n",
    "    return pd.DataFrame(cross_val_results)\n",
    "\n",
    "def train_final_models(model_names, df_combined, df_test):\n",
    "    test_results = []\n",
    "    for model_name in model_names:\n",
    "        print(f\"\\nTraining and evaluating {model_name} on the test set.\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        train_dataset = Dataset.from_pandas(df_combined)\n",
    "        tokenized_train = train_dataset.map(lambda x: preprocess_function(x, tokenizer), batched=True)\n",
    "        data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=4, ignore_mismatched_sizes=True)\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f'/scratch/lf93/iw/cv_results/{model_name}/final',\n",
    "            learning_rate=5e-5,\n",
    "            per_device_train_batch_size=16,\n",
    "            per_device_eval_batch_size=16,\n",
    "            num_train_epochs=3,\n",
    "            weight_decay=0.01,\n",
    "            push_to_hub=False\n",
    "        )\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_train,\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=compute_metrics\n",
    "        )\n",
    "        trainer.train()\n",
    "        test_dataset = Dataset.from_pandas(df_test)\n",
    "        tokenized_test = test_dataset.map(lambda x: preprocess_function(x, tokenizer), batched=True)\n",
    "        test_results_output = trainer.predict(tokenized_test)\n",
    "        test_metrics = compute_metrics(test_results_output)\n",
    "        print(f\"\\nEvaluation for model {model_name} on the test set:\")\n",
    "        print(f\"Accuracy: {test_metrics['accuracy']:.2f}\")\n",
    "        print(f\"Precision: {test_metrics['precision']:.2f}\")\n",
    "        print(f\"Recall: {test_metrics['recall']:.2f}\")\n",
    "        print(f\"F1 Score: {test_metrics['f1']:.2f}\")\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(test_metrics['confusion_matrix'])\n",
    "        test_results.append({\n",
    "            'Model': model_name,\n",
    "            'Test Accuracy': test_metrics['accuracy'],\n",
    "            'Test F1 Score': test_metrics['f1'],\n",
    "            'Test Precision': test_metrics['precision'],\n",
    "            'Test Recall': test_metrics['recall'],\n",
    "            'Test Confusion Matrix': test_metrics['confusion_matrix']\n",
    "        })\n",
    "    return pd.DataFrame(test_results)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70d83aa-1196-46c9-bd34-6ef3c44dacaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "df_combined, df_test = load_data()\n",
    "\n",
    "model_names = [\n",
    "    \"w11wo/javanese-bert-small-imdb-classifier\",\n",
    "    \"w11wo/javanese-gpt2-small-imdb-classifier\",\n",
    "    \"w11wo/javanese-distilbert-small-imdb-classifier\"\n",
    "]\n",
    "\n",
    "cross_val_results_df = cross_validate_models(model_names, df_combined, k_folds=5)\n",
    "print(\"\\nCross-Validation Results:\")\n",
    "print(cross_val_results_df)\n",
    "\n",
    "test_results_df = train_final_models(model_names, df_combined, df_test)\n",
    "print(\"\\nTest Set Evaluation Results:\")\n",
    "print(test_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b00fb9e-5171-45a5-a7ad-8ce0c0190b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f5adb3-0199-4dde-9d04-f88962584e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
