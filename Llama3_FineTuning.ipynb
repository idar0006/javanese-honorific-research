{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c21f49e-bd01-476e-992e-1d63eba52bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc10f70-3dc3-4d81-bd04-b9d6c680a680",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c2433b-8423-447b-802c-2251c852e98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import bitsandbytes as bnb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, PeftConfig\n",
    "from trl import SFTTrainer\n",
    "from trl import setup_chat_format\n",
    "from transformers import (AutoModelForCausalLM, \n",
    "                          AutoTokenizer, \n",
    "                          BitsAndBytesConfig, \n",
    "                          TrainingArguments, \n",
    "                          pipeline, \n",
    "                          logging)\n",
    "from sklearn.metrics import (accuracy_score, \n",
    "                             classification_report, \n",
    "                             confusion_matrix)\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf1d3d8-dcb2-4654-8307-684b785ceade",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"pytorch version {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c8d3b1-eab1-445b-868d-e9786dc813ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"working on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b089c9c-b10e-4a9a-adec-870a724e92b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21b0e6f-b982-48e1-b927-318a93c64a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"/scratch/lf93/iw/df_all_group3.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc6cd35-62bb-44cc-9e6a-d01339561e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "df = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f3907b-5fcc-4c0a-98da-43ee33b5822a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [\"id\", \"label\", \"text\", \"group\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680eab0d-ad04-48f5-aa74-0a4abbdea99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38211fa-cd5d-4dd8-b8ca-4b0796c8bfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 60% training, 20% testing, 20% evaluation\n",
    "train_size = 0.6\n",
    "test_size = 0.2\n",
    "\n",
    "# Initialize empty lists for training, testing, and evaluation data\n",
    "X_train_list = []\n",
    "X_test_list = []\n",
    "X_eval_list = []\n",
    "\n",
    "# Split data by label\n",
    "for label in df['label'].unique():\n",
    "    label_data = df[df.label == label]\n",
    "    train, temp = train_test_split(label_data, train_size=train_size, random_state=42)\n",
    "    test, eval = train_test_split(temp, test_size=0.5, random_state=42)  # Split remaining 40% into 20% test and 20% eval\n",
    "    \n",
    "    X_train_list.append(train)\n",
    "    X_test_list.append(test)\n",
    "    X_eval_list.append(eval)\n",
    "\n",
    "# Concatenate the lists to form the final DataFrames\n",
    "X_train = pd.concat(X_train_list).sample(frac=1, random_state=10).reset_index(drop=True)\n",
    "X_test = pd.concat(X_test_list).reset_index(drop=True)\n",
    "X_eval = pd.concat(X_eval_list).reset_index(drop=True)\n",
    "\n",
    "def generate_prompt(data_point, include_label=False):\n",
    "    prompt = f\"\"\"\n",
    "        Analyze the Javanese sentence enclosed in square brackets. Determine if it is ngoko, ngoko alus, krama, or krama alus, \n",
    "        and return the answer as the corresponding text label: 0 (ngoko), 1 (ngoko alus), 2 (krama), or 3 (krama alus). \n",
    "        Provide only the integer label without any additional explanation. \n",
    "        [{data_point[\"text\"]}]\n",
    "    \"\"\".strip()\n",
    "    if include_label:\n",
    "        prompt += f\" = {data_point['label']}\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# Generate the prompts for training and evaluation\n",
    "#X_train = pd.DataFrame(X_train.apply(lambda x: generate_prompt(x, include_label=True), axis=1), columns=[\"text\"])\n",
    "#X_eval = pd.DataFrame(X_eval.apply(lambda x: generate_prompt(x, include_label=True), axis=1), columns=[\"text\"])\n",
    "\n",
    "# Extract labels for the test set and generate test prompts\n",
    "#y_true = X_test.label\n",
    "#X_test = pd.DataFrame(X_test.apply(lambda x: generate_prompt(x, include_label=False), axis=1), columns=[\"text\"])\n",
    "\n",
    "# Convert DataFrames to Hugging Face Datasets\n",
    "#train_data = Dataset.from_pandas(X_train)\n",
    "#eval_data = Dataset.from_pandas(X_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94facf32-e7c8-4ed5-b5e1-8909e9c56027",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.to_csv(\"df_test4.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39aaf3cf-3128-46d0-ae64-7b5170581686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine train and validation datasets\n",
    "df_combined = pd.concat([X_train, X_eval], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b74804-3157-4507-aa7b-a176a97f7e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Dataset.from_pandas(df_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce0ee86-fea0-48ae-9033-fa6cc462dad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80eaa122-3ff6-429d-985e-f72ce90d9a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the full text of a specific row by its index\n",
    "row_index = 1  # Change this to the index of the row you want to inspect\n",
    "print(X_test.iloc[row_index]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6768505-82a9-4feb-ba85-919f7cb26b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_combined.iloc[1000]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d212b6-46b0-4d56-818c-8b8807e98d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "access_token = \"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\"\n",
    "cache_dir = \"/scratch/lf93/iwand/.cache/huggingface/transformers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839712d8-bef2-4e94-b6ce-cda2379e49cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=device,\n",
    "    torch_dtype=compute_dtype,\n",
    "    quantization_config=bnb_config, \n",
    "    token=access_token, cache_dir=cache_dir\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "max_seq_length = 512 #2048\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, max_seq_length=max_seq_length, token=access_token, cache_dir=cache_dir)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcd3974-1381-48a5-9085-07caef0c998e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer = tokenizer,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=device,\n",
    ")\n",
    "\n",
    "def predict(test, pipeline):\n",
    "    y_pred = []\n",
    "    terminators = [\n",
    "        pipeline.tokenizer.eos_token_id,\n",
    "        pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "    \n",
    "    for prompt in tqdm(test[\"text\"]):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an AI assistant specialized in Javanese language classification.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "        input_ids = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "\n",
    "        terminators = [\n",
    "            tokenizer.eos_token_id,\n",
    "            tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "        ]\n",
    "\n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=256,\n",
    "            eos_token_id=terminators,\n",
    "            do_sample=True,\n",
    "            temperature=0.6,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "    \n",
    "        response = outputs[0][input_ids.shape[-1]:]\n",
    "        decoded_response = tokenizer.decode(response, skip_special_tokens=True).strip()\n",
    "    \n",
    "        # Print the response for verification\n",
    "        print(f\"prompt: {prompt}\")\n",
    "        print(f\"Model Response: {decoded_response}\")\n",
    "    \n",
    "        # Extract the predicted label from the model response using regex\n",
    "        match = re.search(r'\\b[0-3]\\b', decoded_response)\n",
    "        predicted_label = int(match.group()) if match else None\n",
    "\n",
    "        y_pred.append(predicted_label)\n",
    "        \n",
    "    \n",
    "    return y_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e764c19-b0b0-4d82-898f-ea3b8aa035f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = predict(X_test, pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbf7b38-9789-4a4a-81db-094632d1309c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bc2c94-1084-40fb-80e3-154579f909e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f11c8e5-bf20-4d21-97d6-c8c4a50e02fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e167abf-167d-4535-91db-51fac7d9b1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559208fb-0169-4dbf-bb7f-9b3455eab8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = pd.DataFrame({'text': X_test[\"text\"], \n",
    "                           'y_true':y_true, \n",
    "                           'y_pred': y_pred},\n",
    "                         )\n",
    "evaluation.to_csv(\"test_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9cc726-b9d5-4881-9aec-9bf995d8bac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (accuracy_score, \n",
    "                             recall_score, \n",
    "                             precision_score, \n",
    "                             f1_score)\n",
    "\n",
    "from transformers import EarlyStoppingCallback, IntervalStrategy\n",
    "\n",
    "def compute_metrics(p):    \n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "    recall = recall_score(y_true=labels, y_pred=pred)\n",
    "    precision = precision_score(y_true=labels, y_pred=pred)\n",
    "    f1 = f1_score(y_true=labels, y_pred=pred)    \n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b17535-864b-42c8-967d-b5d078cbf271",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir=\"/scratch/lf93/iw/trained_weights\"\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    ")\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,                    # directory to save and repository id\n",
    "    num_train_epochs=5,                       # number of training epochs\n",
    "    per_device_train_batch_size=1,            # batch size per device during training\n",
    "    gradient_accumulation_steps=8,            # number of steps before performing a backward/update pass\n",
    "    gradient_checkpointing=True,              # use gradient checkpointing to save memory\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=0,\n",
    "    logging_steps=25,                         # log every 10 steps\n",
    "    learning_rate=2e-4,                       # learning rate, based on QLoRA paper\n",
    "    weight_decay=0.001,\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,                        # max gradient norm based on QLoRA paper\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,                        # warmup ratio based on QLoRA paper\n",
    "    group_by_length=False,\n",
    "    lr_scheduler_type=\"cosine\",               # use cosine learning rate scheduler\n",
    "    report_to=\"tensorboard\",                  # report metrics to tensorboard\n",
    "    #evaluation_strategy=\"steps\",              # save checkpoint every epoch\n",
    "    #load_best_model_at_end = True,\n",
    "    #eval_steps = 25,\n",
    "    #metric_for_best_model = 'accuracy',\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=train_data,\n",
    "    #eval_dataset=eval_data,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=max_seq_length,\n",
    "    packing=False,\n",
    "    dataset_kwargs={\n",
    "        \"add_special_tokens\": False,\n",
    "        \"append_concat_token\": False,\n",
    "    },\n",
    "    #compute_metrics=compute_metrics,\n",
    "    #callbacks = [EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65176014-1c0f-49ac-8f18-03b029f20b47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3431ccab-6d53-45d7-9b97-83eb1a126bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model and tokenizer\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abf3226-5cc9-47c6-baa4-11c5d52b5096",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = predict(X_test, pipeline)\n",
    "evaluate(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5db25e-eebb-4186-a2fb-c6a7537780f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cce129-c715-446e-8eb7-86c930088c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e709ac49-e370-49de-b9d0-17e1cba8fc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = pd.DataFrame({'text': X_test[\"text\"], \n",
    "                           'y_true':y_true, \n",
    "                           'y_pred': y_pred},\n",
    "                         )\n",
    "evaluation.to_csv(\"test_predictions3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc673c1-b090-4d6a-bf36-283c26e86e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# and the labels are in a column called 'label'\n",
    "label_counts = X_train['label'].value_counts()\n",
    "print(label_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38d56c1-4c51-4f65-a435-ffc7acdd6546",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
