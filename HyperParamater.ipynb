{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, TrainingArguments, Trainer\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Load datasets into Hugging Face format\n",
    "dataset = load_dataset('csv', data_files={'train': 'df_train.csv', 'valid': 'df_valid.csv', 'test': 'df_test.csv'})\n",
    "\n",
    "# Define a function to preprocess data\n",
    "def preprocess_function(examples, tokenizer):\n",
    "    return tokenizer(examples['sentence'], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "# Define metrics function\n",
    "def compute_metrics(p):\n",
    "    preds = p.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(p.label_ids, preds, average='weighted')\n",
    "    accuracy = accuracy_score(p.label_ids, preds)\n",
    "    return {'accuracy': accuracy, 'f1': f1, 'precision': precision, 'recall': recall}\n",
    "\n",
    "# Model names\n",
    "model_names = [\n",
    "    \"w11wo/javanese-bert-small-imdb-classifier\",\n",
    "    \"w11wo/javanese-gpt2-small-imdb-classifier\",\n",
    "    \"w11wo/javanese-distilbert-small-imdb-classifier\"\n",
    "]\n",
    "\n",
    "# Hyperparameters to try\n",
    "learning_rates = [5e-5, 2e-5, 1e-5]\n",
    "batch_sizes = [16, 32]\n",
    "num_epochs = 3\n",
    "\n",
    "results = []\n",
    "\n",
    "# Hyperparameter tuning\n",
    "for model_name in model_names:\n",
    "    for learning_rate in learning_rates:\n",
    "        for batch_size in batch_sizes:\n",
    "            print(f\"\\nTraining and evaluating model: {model_name} with learning_rate={learning_rate} and batch_size={batch_size}\")\n",
    "            \n",
    "            # Load tokenizer and model\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=4, ignore_mismatched_sizes=True)\n",
    "            \n",
    "            # Preprocess data\n",
    "            tokenized_data = dataset.map(lambda x: preprocess_function(x, tokenizer), batched=True)\n",
    "            tokenized_data = tokenized_data.rename_column(\"label\", \"labels\")  # Rename label column to 'labels' for Trainer\n",
    "            \n",
    "            data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "            \n",
    "            # Define training arguments\n",
    "            training_args = TrainingArguments(\n",
    "                output_dir=f'./results/{model_name}/lr_{learning_rate}_bs_{batch_size}',\n",
    "                learning_rate=learning_rate,\n",
    "                per_device_train_batch_size=batch_size,\n",
    "                per_device_eval_batch_size=batch_size,\n",
    "                num_train_epochs=num_epochs,\n",
    "                weight_decay=0.01,\n",
    "                evaluation_strategy=\"epoch\",\n",
    "                save_strategy=\"epoch\",\n",
    "                load_best_model_at_end=True,\n",
    "                push_to_hub=False\n",
    "            )\n",
    "            \n",
    "            # Initialize Trainer\n",
    "            trainer = Trainer(\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                train_dataset=tokenized_data[\"train\"],\n",
    "                eval_dataset=tokenized_data[\"valid\"],\n",
    "                tokenizer=tokenizer,\n",
    "                data_collator=data_collator,\n",
    "                compute_metrics=compute_metrics\n",
    "            )\n",
    "            \n",
    "            # Train the model\n",
    "            trainer.train()\n",
    "            \n",
    "            # Evaluate on validation set using predict to get predictions\n",
    "            validation_results = trainer.predict(tokenized_data[\"valid\"])\n",
    "            metrics = compute_metrics(validation_results)\n",
    "            \n",
    "            # Store results\n",
    "            results.append({\n",
    "                'Model': model_name,\n",
    "                'Learning Rate': learning_rate,\n",
    "                'Batch Size': batch_size,\n",
    "                'Accuracy': metrics['accuracy'],\n",
    "                'F1 Score': metrics['f1'],\n",
    "                'Precision': metrics['precision'],\n",
    "                'Recall': metrics['recall']\n",
    "            })\n",
    "\n",
    "# Create a DataFrame for the results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the results table\n",
    "print(\"\\nHyperparameter Tuning Results:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
