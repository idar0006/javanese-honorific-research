{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c3d5a0-0a07-4a99-afd3-f1a0d45a8e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import (accuracy_score, \n",
    "                             classification_report, \n",
    "                             confusion_matrix)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dropout, Dense\n",
    "from tensorflow.keras import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab91137-57ff-475e-a3ae-5b72adf0f01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load data with different encodings\n",
    "def load_data(file_path):\n",
    "    encodings = ['utf-8', 'latin1', 'ISO-8859-1']\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            return pd.read_csv(file_path, encoding=encoding)\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    raise ValueError(\"Failed to decode the file with tried encodings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d39e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "df = load_data('df_all_group3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1898b8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [\"id\", \"label\", \"text\", \"group\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080d3a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50950e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y = pd.get_dummies(df.label, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164e8859",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Initialize lists to store the split data\n",
    "X_train_list = []\n",
    "X_test_list = []\n",
    "X_eval_list = []\n",
    "y_train_list = []\n",
    "y_test_list = []\n",
    "y_eval_list = []\n",
    "\n",
    "# Split data by label\n",
    "for label in df['label'].unique():\n",
    "    label_data = df[df.label == label]\n",
    "    label_target = df_y[df.label == label]\n",
    "\n",
    "    # Split 60% for training\n",
    "    train, temp, train_target, temp_target = train_test_split(\n",
    "        label_data, \n",
    "        label_target, \n",
    "        train_size=0.6, \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Split remaining 40% into 20% test and 20% eval\n",
    "    test, eval, test_target, eval_target = train_test_split(\n",
    "        temp, \n",
    "        temp_target, \n",
    "        test_size=0.5, \n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    X_train_list.append(train)\n",
    "    X_test_list.append(test)\n",
    "    X_eval_list.append(eval)\n",
    "    y_train_list.append(train_target)\n",
    "    y_test_list.append(test_target)\n",
    "    y_eval_list.append(eval_target)\n",
    "\n",
    "# Concatenate and shuffle training data\n",
    "X_train = pd.concat(X_train_list).sample(frac=1, random_state=10)\n",
    "y_train = pd.concat(y_train_list).loc[X_train.index]\n",
    "\n",
    "# Concatenate test and eval data\n",
    "X_test = pd.concat(X_test_list)\n",
    "y_test = pd.concat(y_test_list)\n",
    "X_eval = pd.concat(X_eval_list)\n",
    "y_eval = pd.concat(y_eval_list)\n",
    "\n",
    "# Ensure eval set has equal representation of each label, if necessary\n",
    "X_eval = (X_eval\n",
    "          .groupby('label', group_keys=False)\n",
    "          .apply(lambda x: x.sample(n=min(len(x), 50), random_state=10, replace=True)))\n",
    "y_eval = df_y.loc[X_eval.index]\n",
    "\n",
    "# Reset index for the final datasets\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)\n",
    "X_eval.reset_index(drop=True, inplace=True)\n",
    "y_eval.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385b5b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 10_000\n",
    "max_len = 128\n",
    "\n",
    "def tokenize_pad_sequences(text, tokenizer=None):\n",
    "    '''\n",
    "    This function tokenize the input text into sequences of intergers and then\n",
    "    pad each sequence to the same length\n",
    "    '''\n",
    "    # Text tokenization\n",
    "    if tokenizer is None:\n",
    "        tokenizer = Tokenizer(num_words=max_words, lower=True, split=' ')\n",
    "        tokenizer.fit_on_texts(text)\n",
    "    # Transforms text to a sequence of integers\n",
    "    X = tokenizer.texts_to_sequences(text)\n",
    "    # Pad sequences to the same length\n",
    "    X = pad_sequences(X, padding='post', maxlen=max_len)\n",
    "    # return sequences\n",
    "    return X, tokenizer\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words, lower=True, split=' ')\n",
    "tokenizer.fit_on_texts(X_train.text)\n",
    "\n",
    "\n",
    "X_train, tokenizer = tokenize_pad_sequences(X_train.text)\n",
    "X_eval, _ = tokenize_pad_sequences(X_eval.text,tokenizer)\n",
    "X_test, _ = tokenize_pad_sequences(X_test.text,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1060cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10_000\n",
    "embedding_size = 64\n",
    "max_len = 128\n",
    "\n",
    "inputs = Input(shape=(max_len,))\n",
    "embedding_layer = Embedding(vocab_size, embedding_size, input_length=max_len)(inputs)\n",
    "conv1d_layer = Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(embedding_layer)\n",
    "maxpooling_layer = MaxPooling1D(pool_size=2)(conv1d_layer)\n",
    "bidirectional_lstm_layer = Bidirectional(LSTM(32))(maxpooling_layer)\n",
    "dropout_layer = Dropout(0.4)(bidirectional_lstm_layer)\n",
    "outputs = Dense(4, activation='softmax')(dropout_layer) \n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fd1dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_data=(X_eval, y_eval),\n",
    "                    batch_size=batch_size, epochs=epochs, verbose=1,\n",
    "                    shuffle=True,\n",
    "                    callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e17249",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f04433",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_true, y_pred):\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_true=y_true, y_pred=y_pred)\n",
    "    print(f'Accuracy: {accuracy:.3f}')\n",
    "    \n",
    "    # Generate accuracy report\n",
    "    unique_labels = set(y_true)  # Get unique labels\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        label_indices = [i for i in range(len(y_true)) \n",
    "                         if y_true[i] == label]\n",
    "        label_y_true = [y_true[i] for i in label_indices]\n",
    "        label_y_pred = [y_pred[i] for i in label_indices]\n",
    "        accuracy = accuracy_score(label_y_true, label_y_pred)\n",
    "        print(f'Accuracy for label {label}: {accuracy:.3f}')\n",
    "        \n",
    "    # Generate classification report\n",
    "    class_report = classification_report(y_true=y_true, y_pred=y_pred)\n",
    "    print('\\nClassification Report:')\n",
    "    print(class_report)\n",
    "    \n",
    "    # Generate confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_true=y_true, y_pred=y_pred, labels=[0, 1, 2, 3])\n",
    "    print('\\nConfusion Matrix:')\n",
    "    print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19e83fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(y_true=np.argmax(y_test, axis=1), y_pred=np.argmax(y_pred, axis=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
